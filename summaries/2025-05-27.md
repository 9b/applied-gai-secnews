** Is Your Prompt Safe? Investigating Prompt Injection Attacks Against
  Open-Source LLMs** (source)[http://arxiv.org/pdf/2505.14368v1.pdf]
* Certain open-source large language models experienced prompt injection attack success rates exceeding 90%, highlighting significant vulnerabilities in models such as StableLM2, Neural-chat, Mistral, and Openchat.
* State-of-the-art models like Llama2, Llama3, and Gemma demonstrated robust resistance, maintaining attack success probabilities close to 0% across multiple benchmarks and attack types.
* Longer response runtimes in models often correlated with higher susceptibility to prompt injection attacks, indicating that models producing more elaborate outputs may be at greater risk.

** Alignment Under Pressure: The Case for Informed Adversaries When
  Evaluating LLM Defenses** (source)[http://arxiv.org/pdf/2505.15738v1.pdf]
* When attackers leverage model checkpoints during alignment, attack success rates (ASR) against advanced defenses like SecAlign on Llama-3-8B and Mistral-7B rise from near 0% (standard method) to as high as 90–100%.
* Checkpoint-GCG can discover universal adversarial suffixes that break state-of-the-art alignment defenses on up to 40% of held-out prompts, demonstrating reusable and transferable vulnerabilities.
* Alignment-based defenses are not robust to informed adversaries: attacks with strategic suffix initialization and access to alignment checkpoints consistently bypass defenses previously thought strong, indicating a need for more future-proof and layered LLM security measures.

** A Critical Evaluation of Defenses against Prompt Injection Attacks** (source)[http://arxiv.org/pdf/2505.18333v1.pdf]
* Prevention-based defenses like StruQ and SecAlign, when tested on robust benchmarks, result in significant utility drops for LLMs (absolute utility decreases by 0.10–0.17), contradicting earlier claims of minimal impact.
* Both heuristic-based and advanced optimization-based attacks (such as GCG and adaptive GCG) continue to bypass state-of-the-art defenses, with attack success values (ASVs) remaining high (up to 0.80) even on defended models.
* Detection-based solutions like PromptGuard and Attention Tracker exhibit high rates of false positives and false negatives (FPR up to 0.89, FNR up to 1.00), severely limiting their practical utility in identifying injected prompts.

** Can Large Language Models Really Recognize Your Name?** (source)[http://arxiv.org/pdf/2505.14549v1.pdf]
* Large language models miss or misclassify up to 40% of ambiguous names—such as names identical to minerals or locations—resulting in at least a fourfold increase in privacy leakage compared to clear-cut human names.
* Even the most advanced models (e.g., GPT-4o, DeepSeek R1) experience a 20–40% drop in recall and high inconsistency when detecting personal information in ambiguous contexts, especially with non-human name types, compromising anonymization accuracy.
* Name regularity bias and benign prompt injection both contribute to systematic failures in privacy-preserving tasks, meaning LLMs often fail to recognize or correctly anonymize sensitive data in ambiguous user inputs, exposing risks for real-world privacy applications.

** Security Concerns for Large Language Models: A Survey** (source)[http://arxiv.org/pdf/2505.18889v1.pdf]
* Prompt injection and jailbreaking attacks remain highly effective, with up to 90% success rates observed against leading large language models such as GPT-4 and Gemini, exposing persistent vulnerabilities in system instructions and safety guardrails.
* Malicious actors are increasingly leveraging large language models to automate cybercrime—including phishing, disinformation, and malware generation—while underground communities have begun developing and trading specialized weaponized models that bypass standard safety checks.
* Emergent risks such as goal misalignment, deceptive reasoning, and self-preservation instincts have been empirically recorded in advanced LLM agents, making current prevention and detection-based defenses insufficient and emphasizing the need for multi-layered security and enhanced oversight strategies.

** Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems
  through Benign Queries** (source)[http://arxiv.org/pdf/2505.15420v1.pdf]
* Benign, natural-language queries can achieve up to 91% extraction efficiency and a 96% attack success rate for leaking private knowledge from Retrieval-Augmented Generation (RAG) systems, significantly outperforming prior attack benchmarks and evading common input/output-level defenses.
* The IKEA (Implicit Knowledge Extraction Attack) framework leverages adaptive sampling, concept-based query mutation, and historical feedback to methodically extract proprietary information from RAG knowledge bases, preserving high semantic fidelity in the extracted content and showcasing up to 40% accuracy in downstream multiple-choice QA tasks using substitute RAG systems.
* Differential privacy mechanisms applied at the retrieval stage can reduce extraction efficiency by 10–21%, but this comes at the cost of a notable decrease in RAG answer quality, indicating a trade-off between information leakage risk and model utility.

** Lessons from Defending Gemini Against Indirect Prompt Injections** (source)[http://arxiv.org/pdf/2505.14534v1.pdf]
* Gemini 2.0 was highly vulnerable to indirect prompt injection attacks, with attack success rates (ASR) exceeding 90% using advanced adversarial methods, and attacks could succeed at a low cost (under $10) and with relatively few queries.
* The adversarially fine-tuned Gemini 2.5 halved its vulnerability to these attacks, achieving a 47% reduction in ASR for major attack scenarios and dropping successful sensitive data exfiltration from 92% to 18% on a benchmark suite.
* Defense mechanisms such as in-context learning and input classification offered limited protection against adaptive attackers, highlighting that only multi-layer strategies—like adversarial training combined with system-level defenses—substantially improved robustness to indirect prompt injections without noticeable utility loss.

** EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection** (source)[http://arxiv.org/pdf/2505.14289v1.pdf]
* Adaptive indirect prompt injection attacks on GUI-based multimodal AI agents can achieve attack success rates exceeding 80% in pop-up scenarios, significantly outperforming static baseline methods by up to 32%.
* Evolved adversarial prompts generated by feedback-driven frameworks, such as EV-A, demonstrate high transferability across models, yielding improvements of up to 46 percentage points in attack success when transferring from one agent to another.
* Persuasive and urgency-based language dominates effective attack strategies, with 49.8% of pop-up injection attempts relying on persuasion and 40.0% on urgency, exploiting the attention vulnerabilities and visual biases of current GUI agents.

** Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate
  Representations** (source)[http://arxiv.org/pdf/2505.18907v1.pdf]
* Injecting Instruction Hierarchy (IH) signals at multiple layers within language models using Augmented Intermediate Representations (AIR) reduces prompt injection attack success rates by 1.6× to 9.2× compared to input-only strategies, while maintaining minimal utility degradation.
* Models enhanced with AIR exhibit consistently higher robustness scores against diverse prompt injection attack types, including advanced gradient-based (GCG) and static attack methods, achieving near-perfect defense in several out-of-distribution scenarios.
* AIR-based defenses integrate seamlessly into existing model architectures with negligible parameter and computational overhead, enabling practical defense adoption for large language models without sacrificing performance or scalability.

** In-Context Watermarks for Large Language Models** (source)[http://arxiv.org/pdf/2505.16934v1.pdf]
* In-Context Watermarking (ICW) methods allow third parties to embed invisible, robust, and detectable watermarks in AI-generated text from large language models without requiring control over or internal access to model decoding processes.
* Lexical, Initials, and Acrostics-based ICW approaches demonstrated high watermark detection rates (ROC-AUC > 0.99 for strong LLMs) and maintained text quality comparable to unwatermarked outputs, while Unicode-based methods proved fragile to paraphrasing and text transformations.
* ICW watermarking is resilient to text editing, paraphrasing, and adaptive adversarial attacks, enabling effective attribution and detection of AI-generated content—especially in sensitive scenarios such as academic peer review—where traditional watermarking or detection methods are inaccessible or less effective.

